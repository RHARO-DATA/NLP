{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text\n",
    "\n",
    "It is rarely ever a good idea to work with text data in its raw format. Text data is messy and filled with low-value noise. To built reliable analyses on this type of data, we first need to preprocess the data to reduce this noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Expand the max width of how our dataFrames display on screen\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a sentence to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Overfitting means that a model was trained too well and it's fitting too closely to the training dataset. \n",
    "A model has been overfit when the model is too complex (i.e. too many features/variables compared to the number of observations). \n",
    "An overfit model may achieve greater than 90 percent accuracy on the training set, but will likely perform poorly on the test set.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "no_contractions = contractions.fix(text)\n",
    "\n",
    "pd.DataFrame({\"Before\": [text], \"After\": [no_contractions]}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = no_contractions.lower()\n",
    "\n",
    "pd.DataFrame({\"Before\": [no_contractions], \"After\": [lower_text]}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove digits\n",
    "\n",
    "Using [regular expressions](https://docs.python.org/3/library/re.html), we can identify sequences of characters that follow a particular pattern (i.e., phone numbers, zip codes, phrases that begin/end with a word/character, etc.). In the cell below, we're removing any standalone digits.\n",
    "\n",
    "[Python regex cheat sheet](https://www.dataquest.io/blog/regex-cheatsheet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "no_digits = re.sub(r'\\b\\d+\\b', '', lower_text)\n",
    "\n",
    "pd.DataFrame({\"Before\": [lower_text], \"After\": [no_digits]}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text\n",
    "\n",
    "nltk's `word_tokenize` function is a bit more advanced than the standard `split` function. `Tokenize` views the text linguistically and handles tokenizing compound terms, contractions, and punctuation much better than `split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(no_digits)\n",
    "\n",
    "pd.DataFrame({\"Before\": [no_digits], \"After\": [tokens]}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python comes with a provided set of punctuation characters called `string.punctuation`. This can be very helpful so we don't have to try to remember which punctuation values to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# capture only the tokens that are not a part of the punctuation list\n",
    "no_punctuation = [w for w in tokens if w not in string.punctuation]\n",
    "\n",
    "pd.DataFrame({\"Before\": [tokens], \"After\": [no_punctuation]}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove low-value words\n",
    "\n",
    "In this cell, we're performing 2 tasks:\n",
    "\n",
    "##### 1. Removing stopwords\n",
    "\n",
    "Stopwords are frequently used words that provide very little value to the meaning of the sentence (e.g., 'a', 'the', 'of', 'and', etc.). NLTK comes prepackaged with stopwords lists for various languages. In the cell below, we're storing the stopwords list as `stop_words` and then we are using `stop_words.extend()` to include additional terms that we would like to remove. These additional words would come from your exploration of the data.\n",
    "\n",
    "##### 2. Removing short words\n",
    "\n",
    "Another thing we're doing in this cell is removing words that are less than a given length. In this scenario, we're only keeping words that are at least 3 characters. Again, this is something that you would decide throughout your exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# capture nltk's english stopwords list\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['said', 'would', 'subject', 'use', 'also', 'like', 'know', 'well', ' could', 'thing'])\n",
    "\n",
    "# filter out stopwords and short words\n",
    "no_stopwords = [w for w in no_punctuation if ((w not in stop_words) and (len(w) >= 3))]\n",
    "\n",
    "pd.DataFrame({\"Before\": [no_punctuation], \"After\": [no_stopwords]}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Stemming\n",
    "\n",
    "Stemming is method of reducing inflectional forms of related terms. The goal is to reduce terms down to a root form. Harmonizing variations of a term assists in properly representing a term's presence in the text. \n",
    "\n",
    "[More information on stemming and lemmatization.](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stems = [ps.stem(w) for w in no_stopwords]\n",
    "\n",
    "pd.DataFrame({\"Before\": [no_stopwords], \"After\": [stems]}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Lemmatization\n",
    "\n",
    "[Lemmatization](https://pythonprogramming.net/lemmatizing-nltk-tutorial/) is very similar to stemming, except lemmatization will always return actual english words. This can be helpful when you need to prioritize interpretability of your model's features. The drawback is that lemmatization does not harmonize variations of terms as aggressively as stemming.\n",
    "\n",
    "NLTK's WordNetLemmatizer is built on top of the vast lexical database known as [WordNet](https://wordnet.princeton.edu/). To leverage this lemmatizer, you must provide a Part of Speech identifier (`n` - Noun (default), `v` - Verb, `a` - Adjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "lemmas = []\n",
    "for word in no_stopwords:\n",
    "    clean_word = wn.lemmatize(word, pos='n')\n",
    "    clean_word = wn.lemmatize(clean_word, pos='v')\n",
    "    lemmas.append(clean_word)\n",
    "    \n",
    "pd.DataFrame({\"Before\": [no_stopwords], \"After\": [lemmas]}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before and After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Before\": [text], \"After\": [lemmas]}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess_text\n",
    "\n",
    "pd.DataFrame({\"Before\": [text], \"After\": [preprocess_text(text)]}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RiceData2019] *",
   "language": "python",
   "name": "conda-env-RiceData2019-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
