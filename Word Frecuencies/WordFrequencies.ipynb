{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Word Frequencies\n",
    "\n",
    "\n",
    "Within this notebook, we'll explore some text data and compile the top N most frequently occuring terms within categorical groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text dataset from SKlearn's [`fetch_20newsgroups`](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)\n",
    "\n",
    "SKlearn's [`fetch_20newsgroups`](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) is a pre-compiled dataset that (as its name suggests) offers news data for 20 different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# define which categories we'd like to use\n",
    "topic_categories = [\n",
    "    'alt.atheism',\n",
    "    'comp.graphics',\n",
    "    'comp.sys.ibm.pc.hardware',\n",
    "    'comp.windows.x',\n",
    "    'misc.forsale',\n",
    "    'rec.autos',\n",
    "    'sci.space',\n",
    "    'rec.motorcycles',\n",
    "    'rec.sport.baseball',\n",
    "    'sci.crypt']\n",
    "\n",
    "# remove unnecessary components of each record to focus on just the \n",
    "# text body and filter by categories defined above\n",
    "news = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), categories=topic_categories)\n",
    "\n",
    "# documents\n",
    "docs = news.data\n",
    "\n",
    "# categories\n",
    "cats = news.target\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame({\"body\": docs, \"category\": [news.target_names[x] for x in cats]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many documents per category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pandas `apply` to broadcast the `split` function to every row's _body_ column\n",
    "\n",
    "The pandas `apply` function broadcasts a function over all values of a particular column of a DataFrame (or a Series). Within the `apply` function, `lambda` is acting similar to a JavaScript _arrow_ function. It is an abbreviated way to write a function. Below, `body` represents each row's text value in the _body_ column, and `body.split()` splits the text string by spaces into a list of individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_tokens'] = df['body'].apply(lambda body : body.split())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group dataframe by category and combine each record's list of tokens\n",
    "\n",
    "Performing a _sum_ aggregation on a column that contains lists will merge the lists into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_tokens = df.groupby('category')['body_tokens'].sum()\n",
    "category_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the number of tokens by category\n",
    "\n",
    "Below we're computing 3 metrics:\n",
    "1. **Number of tokens** - Calculated by simply finding the length of each category's list of tokens\n",
    "2. **Number of _unique_ tokens** - Calculated by first reducing the list of tokens down to unique values using the `set` function, then finding the length\n",
    "3. **Lexical Diversity** - Ratio of unique terms to total terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_df = pd.DataFrame({\"Total Number of Tokens\": category_tokens.apply(lambda x: len(x)),\n",
    "                        \"Number of Unique Tokens\": category_tokens.apply(lambda x: len(set(x)))})\n",
    "\n",
    "explore_df[\"Lexical Diversity\"] = explore_df['Number of Unique Tokens'] / explore_df['Total Number of Tokens']\n",
    "\n",
    "explore_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Lexical Diversity\n",
    "\n",
    "Keep in mind that we don't know the origin of this data, or the number of authors that generated the underlying records, so conclusions based purely on the aggregate-level diversity scores may be skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_df['Lexical Diversity'].sort_values().plot(kind=\"barh\", \n",
    "                                                   xlim=(0, max(explore_df['Lexical Diversity'].values)*1.1), \n",
    "                                                   figsize=(10,10), \n",
    "                                                   fontsize=15, \n",
    "                                                   title=\"Lexical Diversity by Category\")\n",
    "# add labels to plot\n",
    "for j, v in enumerate(explore_df['Lexical Diversity'].sort_values()):\n",
    "        plt.text(0.05, j, str(round(v,4)), color='white', fontweight='bold', va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the frequencies of each term in the word lists and return the top n most frequent\n",
    "\n",
    "Below we're using the [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) function which receives an iterable object and returns a dictionary with each unique token's frequency. Then, we're using a combination of `sorted` and `operator.itemgetter` to perform a reverse sort on a dictionary by its values, as opposed to sorting by the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "def wordListToFreqList(wordlist, top_n=10):\n",
    "    \"\"\"Compile a list of all words and their frequency of occurence\"\"\"\n",
    "    \n",
    "    # count each term's number of occurrences\n",
    "    freqDict = Counter(wordlist)\n",
    "    \n",
    "    # sort the frequency dictionary by its values descending and return the items as a list of tuples\n",
    "    sortedFreqs = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return sortedFreqs[:top_n]\n",
    "\n",
    "freqs = category_tokens.apply(lambda tokens: wordListToFreqList(tokens))\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the most frequently occurring terms for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gridspec allows us to dynamically add subplots in grid\n",
    "N = len(freqs.keys())\n",
    "cols = 2\n",
    "rows = int(math.ceil(N / cols))\n",
    "gs = gridspec.GridSpec(rows, cols)\n",
    "\n",
    "# define the figure space for the plots\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(N*2)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "# iterate over number of categories to plot each one's top terms\n",
    "for i in range(N):\n",
    "    \n",
    "    # add a plot to the figure\n",
    "    ax = fig.add_subplot(gs[i])\n",
    "    ax.set_title(f\"Most Frequent Words for: {news.target_names[i]}\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # break the terms and term counts into two lists/tuples\n",
    "    x,y = zip(*freqs[i])\n",
    "    #plot the data\n",
    "    ax.bar(x,y)\n",
    "    # increase x-label font size\n",
    "    plt.xticks(fontsize=14)\n",
    "    # place numeric label on the bar\n",
    "    for j, v in enumerate(y):\n",
    "        ax.text(j, v/2, str(v), color='white', fontweight='bold', ha='center')\n",
    "        \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RiceData2019] *",
   "language": "python",
   "name": "conda-env-RiceData2019-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
